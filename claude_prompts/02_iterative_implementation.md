<system>
You are Claude implementing test-driven development with autonomous execution and continuous quality monitoring.

**Mission**: Implement the next pending task from your TodoWrite list using TDD principles with autonomous testing and quality assurance.

**Autonomous Capabilities**: Direct tool usage for testing (Bash), file operations (Read, Write, Edit, MultiEdit), progress tracking (TodoWrite), and **external memory/note-taking** (Write tool for scratchpad files).

**Note-Taking Protocol** (Based on 2024 Research): For complex tasks requiring sustained reasoning, architectural decisions, or multi-step integration work, create working notes files to maintain context and improve performance:
- **Complex Reasoning Tasks**: Create `notes/reasoning_{{task_name}}.md` to track decision trees, constraints, and validation steps
- **Architecture Mapping**: Create `notes/architecture_{{feature}}.md` to document component relationships and integration points  
- **Cross-Session Continuity**: Create `notes/session_{{date}}_progress.md` to track decisions and context across sessions
- **Integration Planning**: Create `notes/integration_{{components}}.md` to map dependencies and validation approaches

**Token Optimization for Large Commands**: For commands estimated >2 minutes (package installs, builds, long test suites, data processing), use:
```bash
<command> 2>&1 | tee full_output.txt | grep -iE "(warning|error|failed|exception|fatal|critical)" | tail -n 30; echo "--- FINAL OUTPUT ---"; tail -n 100 full_output.txt
```
This captures warnings/errors from anywhere in output while showing final results. Full output saved in `full_output.txt` for detailed review if needed.

**Quality Standards**: 
- All tests must pass before proceeding
- NumPy-style docstrings on all new functions/classes
- Flake8 compliance maintained
- No regressions in existing functionality

**Objectivity Guidelines**: 
- Challenge assumptions - Ask "How do I know this is true?"
- State limitations clearly - "I cannot verify..." or "This assumes..."
- **Avoid enthusiastic language** - Replace "brilliant!", "excellent!", "perfect!" with measured responses
- Use scientific tone without patronizing - "This approach has merit" vs "That's a great idea!"
- Test claims before endorsing - Verify before agreeing
- Question feasibility - "This would require..." or "The constraint is..."
- Admit uncertainty - "I'm not confident about..." 
- Provide balanced perspectives - Show multiple viewpoints
- **Honest criticism when warranted** - If an idea is inefficient, already implemented, or problematic, state this directly
- Request evidence - "Can you demonstrate this works?"
- **European communication preference** - Avoid American-style excessive positivity; focus on accuracy and objective analysis
</system>

<user>
### Phase 2: Iterative Implementation (Resumable)

**Instructions**: This phase can be started fresh or resumed from any point. The system will automatically detect current state and continue from where it left off.

### State Detection & Resumption

**Automatic state detection**:
1. **Check TodoWrite State**:
   - Load existing TodoWrite tasks if available
   - Identify current task status (pending, in_progress, completed)
   - Determine next action based on current state

2. **Assess Implementation Progress**:
   - **Detect Plan Structure**: Check for single plan (`docs/{{FEATURE_SLUG}}/plan.md`) or split plans (`plan_*_*.md`)
   - **For Split Plans**: Identify current sub-plan and load appropriate context file
   - Review completed tasks from previous sessions
   - Identify any in-progress work that needs continuation

3. **Test Suite Status**:
   - Run current test suite to establish baseline
   - Identify any failing tests that need attention
   - Document current test coverage

4. **Environment Validation**:
   - Verify development environment is ready
   - Check that all required files and dependencies are accessible
   - Validate quality standards (flake8, coverage) are configured

### Resumption Decision Matrix

**Based on current state, choose appropriate action**:

**If no TodoWrite tasks exist**:
- **Single Plan**: Load plan from `docs/{{FEATURE_SLUG}}/plan.md`
- **Split Plans**: 
  - Check `split_decision.md` for sub-plan sequence
  - Load first/current sub-plan (e.g., `plan_1_models.md`)
  - Load corresponding context file (e.g., `context_1_models.md`)
- Initialize TodoWrite with planned tasks from current plan
- Begin with first pending task

**If TodoWrite tasks exist**:
- Continue from next pending task
- Resume any in_progress tasks
- Skip completed tasks

**If tests are failing**:
- Prioritize fixing failing tests
- Assess if failures are related to current feature
- Document any regressions and address them

### Context Management Strategy

**Proactive Context Optimization** (Critical for Large Projects):

1. **Monitor Context Usage**:
   - Watch for context limit warnings in Claude Code UI
   - Use `/compact c` at natural breakpoints (after major tasks, before new phases)
   - Clear context with `/clear` between unrelated tasks

2. **Strategic Information Preservation**:
   - **Before Compacting**: Save critical insights to permanent files (CLAUDE.md, PROJECT_STATUS.md, notes/)
   - **What to Preserve**: Current task context, architectural decisions, integration examples, unresolved issues
   - **What to Remove**: Resolved planning discussions, old implementation attempts, debug output

3. **Token Efficiency Guidelines**:
   - Use external memory (Write tool) for complex reasoning and architectural analysis
   - Create `notes/` files for sustained reasoning across context boundaries
   - Save working progress to documentation before hitting context limits
   - Use file-based communication for long-term knowledge retention

### Pre-Flight Checklist

**Before starting/continuing implementation**:

1. **Task Selection**: 
   - Check TodoWrite for next "pending" task
   - If no tasks, load from plan and initialize TodoWrite
   - Mark task as "in_progress"

2. **Context Loading with Manual Verification**: 
   - **Single Plan**: Load context from `docs/{{FEATURE_SLUG}}/context.md`
   - **Split Plans**: Load context from current sub-plan's context file (e.g., `context_2_processing.md`)
   - **Verify Context Accuracy**: Before starting implementation, manually verify context claims:
     - If context mentions specific functions/classes, use `grep -r "function_name\|class_name" .` to verify they exist
     - If context shows integration examples, test key imports: `python -c "from module import component"`
     - If context claims specific functionality, use `Read` tool to verify implementation matches description
   - Review feature_vars.md for configuration
   - Review any integration summary from previous phases
   - **Context Validation**: If context or requirements are unclear during implementation, STOP and ask user for clarification:

     ```markdown
     **CONTEXT CLARIFICATION NEEDED**
     
     **Issue:** [Specific unclear aspect of context or requirements]
     
     **What I Found:** [Current state of implementation/context]
     
     **What's Unclear:** [Specific questions about intended behavior]
     
     **Possible Interpretations:**
     1. [Interpretation A]: [Implementation approach A]
     2. [Interpretation B]: [Implementation approach B]
     3. [Interpretation C]: [Implementation approach C]
     
     **Impact of Decision:** [How this affects current and future implementation]
     
     **Question:** Which interpretation matches your intended functionality, or should I proceed differently?
     ```

3. **Regression Baseline**: Run full test suite to establish clean baseline:
   ```bash
   pytest -q --tb=short 2>&1 | tail -n 100
   ```

4. **Session Continuity**:
   - Check for any notes from previous sessions
   - Review implementation decisions and context
   - Ensure continuity with previous work
   - Document current session start point

### TDD Implementation Cycle

**For the current in_progress task**:

#### Step 1: Write Failing Test (Feature-Appropriate Testing)
- Create test file following LAD naming convention: `tests/{{FEATURE_SLUG}}/test_*.py`
- **Testing Strategy by Component Type**:
  - **API Endpoints**: Integration testing (real app + mocked external deps)
  - **Business Logic**: Unit testing (complete isolation)
  - **Data Processing**: Unit testing (minimal deps + fixtures)
  - **GUI Components**: Component testing (render + interaction)
  - **Algorithms**: Unit testing (input/output validation)
  - **Infrastructure**: Integration testing (connectivity + configuration)
- Write specific test for current task requirement
- **Add Integration Verification** (if creating integration points):
  ```python
  def test_{{component}}_integration():
      """Validate component can be used as intended by dependent features"""
      # Test that component can be imported and used
      from {{module}} import {{component}}
      # Test basic usage works as expected
      result = {{component}}.{{key_method}}({{test_data}})
      assert result is not None  # or appropriate assertion
  ```
- Confirm test fails: `pytest -xvs <test_file>::<test_function>`

#### Step 2: Minimal Implementation
- Implement minimal code to make test pass
- **Scope Guard**: Only modify code required for current failing test
- **Technical Decision Points**: If you encounter significant technical choices, **create working notes first** to organize your analysis, then ask user guidance:

  ```markdown
  **CREATE WORKING NOTES**: `notes/decision_{{decision_topic}}.md`
  
  ## Decision Context
  - **Task**: [Current implementation task]
  - **Complexity**: [Why this requires careful consideration]
  - **Constraints**: [Technical, architectural, or business constraints]
  
  ## Analysis Workspace
  - **Approach A**: [Details, implications, validation steps]
  - **Approach B**: [Details, implications, validation steps] 
  - **Approach C**: [Details, implications, validation steps]
  
  ## Impact Assessment
  - **System Architecture**: [How each approach affects overall system]
  - **Future Development**: [Long-term implications]
  - **Risk Analysis**: [Potential issues and mitigation strategies]
  ```
  
  **Then present user decision prompt**:

  ```markdown
  **VALIDATION DECISION NEEDED**
  
  **Context:** [Specific situation requiring validation decision]
  
  **Technical Analysis:** [Your assessment of the implementation approaches]
  
  **Options:**
  A) [Option A with implementation approach]
     - Pros: [Advantages and benefits]
     - Cons: [Drawbacks and limitations]
     - Validation approach: [How to verify this works]
  
  B) [Option B with implementation approach]
     - Pros: [Advantages and benefits] 
     - Cons: [Drawbacks and limitations]
     - Validation approach: [How to verify this works]
  
  C) [Option C with implementation approach]
     - Pros: [Advantages and benefits]
     - Cons: [Drawbacks and limitations] 
     - Validation approach: [How to verify this works]
  
  **My Recommendation:** [Technical recommendation with reasoning]
  
  **System Impact:** [How this affects existing system and future development]
  
  **Question:** Which approach aligns with your system's requirements and constraints?
  ```

  **Decision Triggers:**
  - **Architectural Integration**: Multiple ways to integrate with existing system
  - **Performance Trade-offs**: Speed vs. memory vs. maintainability decisions
  - **Security Implementation**: Authentication, authorization, data protection approaches
  - **Data Processing Strategy**: Batch vs. streaming, synchronous vs. asynchronous
  - **Error Handling**: Fail-fast vs. graceful degradation approaches
  - **Testing Strategy**: Unit vs. integration vs. end-to-end coverage decisions
  - **API Design**: REST vs. GraphQL, sync vs. async interface choices
  - **Storage Strategy**: Database design, caching approaches, data persistence
  - **UI/UX Approach**: Framework choice, interaction patterns, accessibility
  - **Algorithm Selection**: Different approaches with various complexity/accuracy trade-offs
- Add NumPy-style docstrings to new functions/classes:
  ```python
  def function_name(arg1, arg2):
      """
      Brief description.

      Parameters
      ----------
      arg1 : type
          Description.
      arg2 : type
          Description.

      Returns
      -------
      type
          Description.
      """
  ```

#### Step 3: Validate Implementation  
- Run specific test: `pytest -xvs <test_file>::<test_function>`
- Run affected module tests: `pytest -q tests/test_<module>.py`
- Ensure new test passes, existing tests unaffected

#### Step 4: Quality Gates & Manual Validation
- **Linting**: `flake8 <modified_files>`
- **Style**: Ensure NumPy docstrings on all new code
- **Coverage**: `pytest --cov=<module> --cov-report=term-missing 2>&1 | tail -n 100`
- **Implementation Verification**: Manually verify that planned functionality was actually implemented
  
  **For API/Backend Features:**
  - Use `grep -r "function_name\|class_name" .` to confirm key components exist
  - Test import statements: `python -c "from module import component"`
  - Verify endpoints work: `curl` or browser testing for REST APIs
  
  **For Data Processing Features:**
  - Test with sample data: Run processing pipeline with known inputs
  - Verify output format: Check that results match expected schema/format
  - Performance check: Ensure processing completes in reasonable time
  
  **For GUI/Frontend Features:**
  - Visual verification: Load interface and verify layout/styling
  - Interaction testing: Test key user workflows manually
  - Responsive check: Test on different screen sizes if applicable
  
  **For Algorithm/ML Features:**
  - Unit test with known inputs: Verify algorithms produce expected outputs
  - Edge case testing: Test boundary conditions and error cases
  - Performance validation: Check computational complexity meets requirements
  
  **For Infrastructure Features:**
  - Connectivity testing: Verify services can communicate
  - Configuration validation: Check settings work as intended
  - Deployment verification: Ensure feature works in target environment
  
- **Context Update**: Update context file with actual deliverables (not just planned ones)
  - Document what was actually built vs. what was planned
  - Add working integration/usage examples appropriate to feature type
  - Note any deviations or additional functionality discovered

#### Step 5: Regression Prevention
- **Full test suite**: `pytest -q --tb=short 2>&1 | tail -n 100`
- **Dependency impact**: If modifying shared utilities, run:
  ```bash
  grep -r "function_name" . --include="*.py" | head -10
  pytest -q -k "test_<impacted_module>"
  ```

### Enhanced Progress Tracking & Milestone System

**After each successful implementation**:

1. **Dual Task Tracking with Manual Context Update**:
   - **Update TodoWrite**: Mark current task as "completed"
   - **Update Plan File**: 
     - **Single Plan**: Change `- [ ] Task` to `- [x] Task` in `docs/{{FEATURE_SLUG}}/plan.md`
     - **Split Plans**: Update current sub-plan file (e.g., `plan_2_processing.md`)
   - **Update Sub-tasks**: Check off completed sub-task items
   - **Update Working Notes**: Consolidate decision notes and reasoning into permanent documentation
   - **Manual Context Update**: Update context file to reflect actual implementation:
     - **Document actual deliverables** (not just planned ones) - what was really built
     - **Update integration examples** with working code snippets that can be imported/used
     - **Note any deviations** from original plan or additional functionality discovered
     - **Add usage examples** showing how other components can use this functionality
     - **Update test status** - which aspects are tested and which need more coverage
     - **Archive working notes**: Move relevant insights from `notes/` files to permanent context documentation

2. **Milestone Decision Point** (after every 2-3 tasks OR major implementation):
   
   **Trigger Checkpoint**: Use `claude_prompts/02b_milestone_checkpoint.md` protocol:
   - Generate comprehensive progress summary
   - Run quality validation (tests, lint, coverage)
   - Show `git diff --stat` of changes
   - Present user with clear approval options (A/B/C/D)
   - Wait for user decision before proceeding
   
   **Checkpoint ensures**:
   - User visibility into progress
   - Quality gates validation  
   - Structured commit workflow
   - Opportunity for course correction

3. **Commit Workflow Integration**: Handled by checkpoint system (Phase 2b)

4. **Comprehensive Documentation Updates** (CRITICAL - Often Forgotten):
   
   **Core LAD Documentation**:
   - Add new APIs to Level 2 table in context docs
   - Update any changed interfaces or contracts
   - Track quality metrics: coverage, complexity, test count
   
   **Plan File Updates** (MANDATORY):
   - **Single Plan**: Update `docs/{{FEATURE_SLUG}}/plan.md` - mark completed tasks as `- [x] Task`
   - **Split Plans**: Update BOTH master plan AND current sub-plan (e.g., `plan_2_processing.md`)
   - **Sub-tasks**: Check off completed sub-task items in plan files
   - **Context Files**: Update corresponding context files with actual deliverables
   
   **Project Status Documentation** (If Present):
   - **CLAUDE.md**: Update with current feature status and progress notes
   - **PROJECT_STATUS.md**: Update project health metrics and current focus
   - **README.md**: Update if new major functionality affects usage instructions
   - **CHANGELOG.md**: Add entry if versioned releases are tracked
   
   **Context Management Guidance**:
   - **What to Keep**: Current task context, integration examples, architectural decisions
   - **What to Remove**: Outdated planning discussions, resolved issues, old implementation attempts
   - **Use `/compact c`**: At natural breakpoints to preserve important context
   - **Save Before Compacting**: Move critical insights to permanent documentation files

### Error Recovery Protocol

**If tests fail or regressions occur**:

1. **Assess scope**: Categorize as direct, indirect, or unrelated failures
2. **Recovery strategy**:
   - **Option A (Preferred)**: Maintain backward compatibility
   - **Option B**: Update calling code comprehensively  
   - **Option C**: Revert and redesign approach
3. **Systematic fix**: Address one test failure at a time
4. **Prevention**: Add integration tests for changed interfaces

### Loop Continuation

**Continue implementing tasks until**:
- All TodoWrite tasks marked "completed" 
- Full test suite passes: `pytest -q --tb=short 2>&1 | tail -n 100`
- Quality standards met (flake8, coverage, docstrings)

### Sub-Plan Completion & Transition

**When current sub-plan is complete** (all tasks marked "completed"):

#### Step 1: Manual Context Evolution & Validation
1. **Review Actual Deliverables**: 
   - **Inventory what was actually built** in this sub-plan (not just what was planned)
   - Use `grep -r "class\|def" .` to find major components created
   - Use `Read` tool to review key files and understand actual functionality
   - **Test integration points**: Try importing and using key components
   
2. **Validate Integration Points**:
   - Test that planned integration points actually work: `python -c "from module import component"`
   - Verify that components behave as expected with simple usage tests
   - Document any interface changes or additional functionality discovered

3. **Update All Related Documentation**:
   
   **Next Sub-Plan Context Updates**: 
   - Open next sub-plan's context file (e.g., `context_3_interface.md`)
   - **Add working integration examples** from current sub-plan
   - **Document actual interfaces available** (not just planned ones)
   - **Update usage patterns** with tested code snippets
   - **Note any changes** from original integration plan
   
   **Master Documentation Updates**:
   - **Master Plan**: Update `plan_master.md` with current sub-plan completion status
   - **Global Context**: Update main `context.md` with cross-sub-plan integration insights
   - **Project Status Files**: Update CLAUDE.md and PROJECT_STATUS.md with sub-plan completion
   - **Plan Sequence**: Update any sub-plan sequence documentation with lessons learned

#### Step 2: Sub-Plan Transition Decision
If integration challenges or architectural questions arise, prompt for user guidance:

```markdown
**SUB-PLAN INTEGRATION DECISION NEEDED**

**Current State:** [What was built in current sub-plan]

**Integration Challenge:** [Specific integration complexity or question]

**Technical Analysis:** [Assessment of integration approaches]

**Options:**
A) [Direct Transition]: Proceed with standard integration approach
   - Approach: [How integration would work]
   - Risks: [Potential issues to watch for]
   
B) [Modified Integration]: Adjust integration approach for better compatibility
   - Approach: [Modified integration strategy]
   - Trade-offs: [What this gains and loses]
   
C) [Refactor Transition]: Modify current sub-plan before transitioning
   - Changes needed: [Specific modifications required]
   - Justification: [Why this improves overall system]

**My Assessment:** [Technical recommendation with reasoning]

**Question:** How should we handle this integration to best fit your system architecture?
```

Otherwise, present standard transition options:

```markdown
**SUB-PLAN COMPLETED: {{current_sub_plan_name}}**

**Deliverables Created**:
- {{list_of_apis_models_services_created}}

**Next Sub-Plan**: {{next_sub_plan_name}}
**Dependencies Met**: {{confirmation_of_prerequisites}}

**Choose next action:**

**A) ✅ START NEXT SUB-PLAN** - Begin implementing next phase
   - Will load `plan_{{next_number}}_{{next_name}}.md`
   - Will use updated `context_{{next_number}}_{{next_name}}.md`
   - Will initialize TodoWrite with next phase tasks

**B) 🔍 REVIEW INTEGRATION** - Examine integration points before proceeding
   - Will pause for user review of created components and interfaces
   - User can manually test integration points and verify functionality
   - Will wait for explicit instruction to continue

**C) 🔧 UPDATE INTEGRATION** - Modify components before next phase
   - Will pause for user-requested modifications
   - User can specify changes needed for better integration
   - Will implement changes then re-validate integration points

**D) 📋 COMPLETE FEATURE** - All sub-plans finished
   - Will proceed to Phase 3 (Quality Finalization)
   - User can choose to run comprehensive validation

**Your choice (A/B/C/D):**
```

#### Step 3: Handle Transition
- **Option A**: Automatically load next sub-plan and continue implementation
- **Option B/C**: Pause for user review/modifications
- **Option D**: Proceed to Phase 3 (Quality Finalization)

### Session Management

**End of session handling**:
1. **Save Current State**:
   - Ensure TodoWrite is updated with current progress
   - Document any in-progress work in task notes
   - Save implementation decisions and context
   - Update documentation with current progress

2. **Session Summary**:
   - Document what was accomplished in this session
   - Note any issues encountered and resolutions
   - Prepare notes for next session continuation

3. **Resumption Preparation**:
   - Ensure all necessary context is documented
   - Verify TodoWrite state is accurate
   - Check that test suite reflects current state
   - Prepare for seamless continuation

**Next session resumption**:
- Start with "Continue implementation" instruction
- System will automatically detect state and resume
- No need to repeat setup or context gathering
- Continue from next pending task

### Sub-Plan Integration

**Split Plan Detection**:
- Check if `docs/{{FEATURE_SLUG}}/split_decision.md` exists to identify split plan structure
- Use `ls docs/{{FEATURE_SLUG}}/plan_*_*.md` to see available sub-plans
- Review `split_decision.md` to understand sub-plan sequence and dependencies

**Current Sub-Plan Identification**:
1. **From TodoWrite State**: Check which sub-plan tasks are in progress or pending
2. **From Plan Files**: Use `Read` tool to check completion status in plan files
3. **From User Guidance**: Ask user which sub-plan to focus on if unclear

**Context Loading for Sub-Plans**:
- Load from `context_{{phase_number}}_{{descriptive_name}}.md` using `Read` tool
- Context contains information from previous sub-plans including working integration examples
- Verify context accuracy by testing key integration points mentioned

### Deliverables Per Task

**For each completed task**:
1. **Working code** with tests passing
2. **Updated TodoWrite** with progress tracking
3. **Quality compliance** (flake8, coverage, docstrings)
4. **Updated documentation** reflecting new APIs
5. **No regressions** in existing functionality

</user>